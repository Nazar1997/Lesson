{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MNIST digits classification with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "# import keras\n",
    "%matplotlib inline\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from IPython.display import clear_output, display_html, HTML\n",
    "import contextlib\n",
    "import time\n",
    "import io\n",
    "import urllib\n",
    "import base64\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from umap import UMAP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the data\n",
    "\n",
    "In this task we have 50000 28x28 images of digits from 0 to 9.\n",
    "We will train a classifier on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from util import load_mnist\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist(flatten=True)\n",
    "\n",
    "plt.figure(figsize=[6, 6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.title(\"Label: %i\" % y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28, 28]), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "little_X = []\n",
    "for i in range(10):\n",
    "    little_X.append(X_train[np.where(y_train == i)[0][: n]])\n",
    "    \n",
    "representation = np.array(np.concatenate(little_X))\n",
    "\n",
    "umap = UMAP()\n",
    "umap_resp = umap.fit_transform(representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "for i in range(10):\n",
    "    plt.scatter(umap_resp[i * n:i * n + n, 0], umap_resp[i * n:i * n + n, 1], c='C' + str(i), label = int(i), s=.5)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model\n",
    "\n",
    "Your task is to train a linear classifier $\\vec{x} \\rightarrow y$ with SGD using TensorFlow.\n",
    "\n",
    "You will need to calculate a logit (a linear transformation) $z_k$ for each class: \n",
    "$$z_k = \\vec{x} \\cdot \\vec{w_k} + b_k \\quad k = 0..9$$\n",
    "\n",
    "And transform logits $z_k$ to valid probabilities $p_k$ with softmax: \n",
    "$$p_k = \\frac{e^{z_k}}{\\sum_{i=0}^{9}{e^{z_i}}} \\quad k = 0..9$$\n",
    "\n",
    "We will use a cross-entropy loss to train our multi-class classifier:\n",
    "$$\\text{cross-entropy}(y, p) = -\\sum_{k=0}^{9}{\\log(p_k)[y = k]}$$ \n",
    "\n",
    "where \n",
    "$$\n",
    "[x]=\\begin{cases}\n",
    "       1, \\quad \\text{if $x$ is true} \\\\\n",
    "       0, \\quad \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Cross-entropy minimization pushes $p_k$ close to 1 when $y = k$, which is what we want.\n",
    "\n",
    "Here's the plan:\n",
    "* Flatten the images (28x28 -> 784) with `X_train.reshape((X_train.shape[0], -1))` to simplify our linear model implementation\n",
    "* Use a matrix placeholder for flattened `X_train`\n",
    "* Convert `y_train` to one-hot encoded vectors that are needed for cross-entropy\n",
    "* Use a shared variable `W` for all weights (a column $\\vec{w_k}$ per class) and `b` for all biases.\n",
    "* Aim for ~0.93 validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher-level API:\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_size=40):\n",
    "        super(Net, self).__init__()\n",
    "        # here you construct weights for layers\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # here you describe usage of layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        self.x = self.fc3(x)\n",
    "        return F.log_softmax(self.x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model interface:\n",
    "model = Net()\n",
    "tt = torch.from_numpy(X_train[:10, :].astype(np.float32))\n",
    "output = model(tt)\n",
    "\n",
    "print('Model outputs: \\n', output)\n",
    "# TODO: получите вероятности из output c помощью функций из torch\n",
    "# hint: см документацию к log_softmax\n",
    "probs = model.forward(tt)\n",
    "print('Probs: \\n', torch.exp(probs))\n",
    "\n",
    "# TODO: получите предсказание из output c помощью функций из torch\n",
    "pred = torch.argmax(probs, dim=1)\n",
    "print('Pred: \\n', pred.data.numpy())\n",
    "print('Truth: \\n', y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "\n",
    "# функция для итераций по минибатчам, из первого семинара\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, batchsize=32):\n",
    "    loss_log = []\n",
    "    model.train()\n",
    "    for x_batch, y_batch in iterate_minibatches(X_train, y_train, batchsize=batchsize, shuffle=True):\n",
    "        # data preparation\n",
    "        data = torch.from_numpy(x_batch.astype(np.float32))\n",
    "        target = torch.from_numpy(y_batch.astype(np.int64))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # make a step\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        loss_log.append(loss)\n",
    "    return loss_log\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    loss_log = []\n",
    "    model.eval()\n",
    "    \n",
    "    data = torch.from_numpy(X_val.astype(np.float32))\n",
    "    target = torch.from_numpy(y_val.astype(np.int64))\n",
    "    \n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss_log = loss.item()\n",
    "    return loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(train_history, val_history, title='loss'):\n",
    "    if title == 'loss':\n",
    "        clear_output()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(title)\n",
    "    plt.plot(np.arange(len(train_history)), train_history, label='train')\n",
    "    plt.plot((1 + np.arange(0, len(val_log))) * 1562, val_history, label='val', c='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = []\n",
    "val_log = []\n",
    "\n",
    "model = Net()\n",
    "opt = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "batchsize = 32\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_loss = train(model, opt, batchsize=batchsize)\n",
    "    train_log.extend(train_loss)\n",
    "    \n",
    "    val_loss = np.mean(test(model))\n",
    "    val_log.append(val_loss)    \n",
    "    \n",
    "    plot_history(train_log, val_log, title='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: добавьте подсчет точности\n",
    "def train(model, optimizer, batchsize=32):\n",
    "    loss_log, acc_log = [], []\n",
    "    \n",
    "    model.train()\n",
    "    for x_batch, y_batch in iterate_minibatches(X_train, y_train, batchsize=batchsize, shuffle=True):\n",
    "        # data preparation\n",
    "        data = torch.from_numpy(x_batch.astype(np.float32))\n",
    "        target = torch.from_numpy(y_batch.astype(np.int64))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        acc_loss = torch.mean((torch.argmax(output, dim=1) == target).float())\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # make a step\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        loss_log.append(loss)\n",
    "        acc_log.append(acc_loss.item())\n",
    "    \n",
    "    return loss_log, acc_log\n",
    "\n",
    "\n",
    "# TODO: добавьте подсчет точности:\n",
    "def test(model):\n",
    "    loss_log, acc_log = [], []\n",
    "    model.eval()\n",
    "    \n",
    "    data = torch.from_numpy(X_val.astype(np.float32))\n",
    "    target = torch.from_numpy(y_val.astype(np.int64))\n",
    "    \n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    acc_loss = acc_loss = torch.mean((torch.argmax(output, dim=1) == target).float())\n",
    "    loss_log = loss.item()\n",
    "    \n",
    "    return loss_log, acc_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_log, train_acc_log = [], []\n",
    "val_log, val_acc_log = [], []\n",
    "\n",
    "model = Net()\n",
    "opt = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "batchsize = 32\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    # train\n",
    "    train_loss, acc_los = train(model, opt, batchsize=batchsize)\n",
    "    train_log.extend(train_loss)\n",
    "    train_acc_log.extend(acc_los)\n",
    "    \n",
    "    # test\n",
    "    val_loss, val_acc_loss = test(model)\n",
    "    val_log.append(val_loss)\n",
    "    val_acc_log.append(val_acc_loss)\n",
    "    \n",
    "    # store metrics\n",
    "    \n",
    "    \n",
    "    # plot all metrics (loss and acc for train/val)\n",
    "    plot_history(train_log, val_log, title='loss')\n",
    "    plot_history(train_acc_log, val_acc_log, title='acc')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "\n",
    "little_X = []\n",
    "for i in range(10):\n",
    "    little_X.append(torch.Tensor(X_train[np.where(y_train == i)[0][: n]]))\n",
    "    \n",
    "model.eval()\n",
    "model(torch.cat(little_X))\n",
    "representation = model.x.detach().numpy()\n",
    "\n",
    "umap = UMAP()\n",
    "umap_resp = umap.fit_transform(representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.grid()\n",
    "for i in range(10):\n",
    "    plt.scatter(umap_resp[i * n:i * n + n, 0], umap_resp[i * n:i * n + n, 1], c='C' + str(i), label = int(i), s=.5)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "                      nn.Conv2d(1,10,3),\n",
    "                      nn.MaxPool2d(3),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(10,16,3),\n",
    "                      nn.MaxPool2d(3, 1),\n",
    "                      nn.ReLU()\n",
    "                    )\n",
    "        self.classifier = nn.Sequential(\n",
    "                      nn.Linear(4 * 4 * 16, 32),\n",
    "                      nn.Linear(32, 16),\n",
    "                      nn.Linear(16, 10)\n",
    "                    )\n",
    "    def forward(self, x):\n",
    "        out = self.features(x.reshape((-1, 1, 28, 28)))\n",
    "        self.out = self.classifier(out.reshape(-1, 4 * 16 * 4))\n",
    "        return F.log_softmax(self.out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = []\n",
    "val_log = []\n",
    "\n",
    "model = Net()\n",
    "opt = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "batchsize = 32\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_loss = train(model, opt, batchsize=batchsize)\n",
    "    train_log.extend(train_loss)\n",
    "    \n",
    "    val_loss = test(model)\n",
    "    val_log.append(val_loss)\n",
    "    \n",
    "    plot_history(np.array(train_log)[0::2, :].flatten(), np.array(val_log)[:,0], title='loss')\n",
    "    plot_history(np.array(train_log)[1::2, :].flatten(), np.array(val_log)[:,1], title='acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to show data representation on your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "\n",
    "little_X = []\n",
    "for i in range(10):\n",
    "    little_X.append(torch.Tensor(X_train[np.where(y_train == i)[0][: n]]))\n",
    "    \n",
    "model.eval()\n",
    "model(torch.cat(little_X))\n",
    "representation = model.x.detach().numpy()\n",
    "\n",
    "umap = UMAP()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_mnist(flatten=False):\n",
    "    \"\"\"taken from https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\"\"\"\n",
    "    # We first define a download function, supporting both Python 2 and 3.\n",
    "    if sys.version_info[0] == 2:\n",
    "        from urllib import urlretrieve\n",
    "    else:\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "    # We then define functions for loading MNIST images and labels.\n",
    "    # For convenience, they also download the requested files if needed.\n",
    "    import gzip\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the inputs in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        # (Actually to range [0, 255/256], for compatibility to the version\n",
    "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "        return data / np.float32(256)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    if flatten:\n",
    "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
    "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
    "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def plot_embedding(X, y):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "    plt.xticks([]), plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
